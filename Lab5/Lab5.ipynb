{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6613a72a",
   "metadata": {},
   "source": [
    "# Metrics in text\n",
    "## Paweł Kruczkiewicz\n",
    "#### 04.05.2021 r.\n",
    "\n",
    "Zadanie dotyczy różnych metryk w przestrzeni napisów.\n",
    "\n",
    "1. Zaimplementuj przynajmniej 3 metryki spośród wymienionych: cosinusowa, LCS, DICE, euklidesowa.\n",
    "2. Zaimplementuj przynajmniej 2 sposoby oceny jakości klasteryzacji (np. indeks Daviesa-Bouldina).\n",
    "3. Stwórz stoplistę najczęściej występujących słów.\n",
    "4. Wykonaj klasteryzację zawartości załączonego pliku (lines.txt) przy użyciu przynajmniej ~~2 algorytmów~~ 1 algorytmu oraz metryk zaimplementowanych w pkt. 1. i metryki Levenshteina. Każda linia to adres pocztowy firmy, różne sposoby zapisu tego samego adresu powinny się znaleźć w jednym klastrze.\n",
    "5. Porównaj jakość wyników sposobami zaimplementowanymi w pkt. 2.\n",
    "6. Czy masz jakiś pomysł na poprawę jakości klasteryzacji w tym zadaniu?\n",
    "\n",
    "Sprawozdanie powinno zawierać porównanie wyników wszystkich metryk z użyciem stoplisty i bez.\n",
    "Można jako wzorcową klasteryzację użyć pliku clusters.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce11cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN  \n",
    "from math import sqrt, inf\n",
    "\n",
    "FILE_PATH = \"lines.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0e4aee",
   "metadata": {},
   "source": [
    "## Zad 1 - Metryki\n",
    "### LCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba3d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs(text_a, text_b):\n",
    "    m, n = len(text_a), len(text_b)\n",
    "    similarities = [[0 for _ in range(n+1)] for _ in range(m+1)]\n",
    "\n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            if text_a[i-1] == text_b[j-1]:\n",
    "                similarities[i][j] = similarities[i-1][j-1] + 1\n",
    "            else:\n",
    "                similarities[i][j] = similarities[i - 1][j - 1]\n",
    "    f = max([max(row) for row in similarities])\n",
    "    return 1 - f/max(m, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94075b92",
   "metadata": {},
   "source": [
    "### DICE\n",
    "`k_dice` zwraca funkcję `dice` z ustalonym paramtrem `k`. Pozwala to zautomatyzowac testy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7292a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(text_a, text_b, k):\n",
    "    def make_n_grams_set(text):\n",
    "        return {text[i:i + k] for i in range(len(text) - k + 1)}\n",
    "\n",
    "    set_a = make_n_grams_set(text_a)\n",
    "    set_b = make_n_grams_set(text_b)\n",
    "    common_digrams = len(set_a.intersection(set_b))\n",
    "    len_a, len_b = len(set_a), len(set_b)\n",
    "\n",
    "    return 1 - 2 * common_digrams / (len_a + len_b)\n",
    "\n",
    "\n",
    "def k_dice(k):\n",
    "    def dice_metric(text_a, text_b):\n",
    "        return dice(text_a, text_b, k)\n",
    "    return dice_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f4575",
   "metadata": {},
   "source": [
    "### Cosinus\n",
    "`k_cosine` dziala analogicznie do `k_dice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ce856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(text_a, text_b, k):\n",
    "    def make_n_gram_dict(text):\n",
    "        result_dict = dict()\n",
    "        for i in range(len(text) - k + 1):\n",
    "            n_gram = text[i: i + k]\n",
    "            result_dict.setdefault(n_gram, 0)\n",
    "            result_dict[n_gram] += 1\n",
    "        return result_dict\n",
    "\n",
    "    def vec_sum(dict_vec):\n",
    "        return sqrt(sum(map(lambda x: x**2, dict_vec.values())))\n",
    "\n",
    "    dict_a, dict_b = make_n_gram_dict(text_a), make_n_gram_dict(text_b)\n",
    "    len_a, len_b = vec_sum(dict_a), vec_sum(dict_b)\n",
    "\n",
    "    scalar_value = 0\n",
    "    for key_a, val_a in dict_a.items():\n",
    "        if key_a in dict_b:\n",
    "            val_b = dict_b[key_a]\n",
    "            scalar_value += val_a * val_b\n",
    "\n",
    "    return 1 - scalar_value/(len_a*len_b)\n",
    "\n",
    "def k_cosine(k):\n",
    "    def cosine_metric(text_a, text_b):\n",
    "        return cosine(text_a, text_b, k)\n",
    "    return cosine_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b082727",
   "metadata": {},
   "source": [
    "### Odległość edycyjna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe3dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(text_a, text_b):\n",
    "    delta = lambda x, y: 0 if x == y else 1\n",
    "\n",
    "    len_a = len(text_a)\n",
    "    len_b = len(text_b)\n",
    "\n",
    "    dist_table = [[0 for _ in range(len_b + 1)] for _ in range(len_a + 1)]\n",
    "\n",
    "    for i in range(1, len_a + 1):\n",
    "        dist_table[i][0] = i\n",
    "    for j in range(1, len_b + 1):\n",
    "        dist_table[0][j] = j\n",
    "\n",
    "    for i in range(1, len_a + 1):\n",
    "        for j in range(1, len_b + 1):\n",
    "            x, y = text_a[i - 1], text_b[j - 1]  # current letters\n",
    "\n",
    "            up_cost, left_cost, diag_cost = dist_table[i - 1][j] + 1, dist_table[i][j - 1] + 1, \\\n",
    "                                            dist_table[i - 1][j - 1] + delta(x, y)\n",
    "            dist_table[i][j] = min(up_cost, left_cost, diag_cost)\n",
    "\n",
    "    return dist_table[len_a][len_b]/max(len_a, len_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab53ec5",
   "metadata": {},
   "source": [
    "## Zad 2 - ocena miary klasteryzacji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e391da",
   "metadata": {},
   "source": [
    "**Funkcje pomocnicze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e68f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_between_elems(cluster, dist):\n",
    "    return [[dist(elem_a, elem_b) for elem_a in cluster] for elem_b in cluster]\n",
    "\n",
    "\n",
    "def sigma(cluster, dist):\n",
    "    distances = dist_between_elems(cluster, dist)\n",
    "    return sum([sum(distances[i]) for i in range(len(distances))])/len(cluster)**2\n",
    "\n",
    "\n",
    "def centroid(cluster):\n",
    "    return cluster[0]\n",
    "\n",
    "\n",
    "def dist_between_clusters(clust_a, clust_b, dist):\n",
    "    centr_a = centroid(clust_a)\n",
    "    centr_b = centroid(clust_b)\n",
    "    return dist(centr_a, centr_b)\n",
    "\n",
    "\n",
    "def size_of_cluster(cluster, dist):\n",
    "    distances = dist_between_elems(cluster, dist)\n",
    "    return max([max(distances[i]) for i in range(len(distances))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189961a0",
   "metadata": {},
   "source": [
    "**Davies-Bouldin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c586b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def davies_bouldin(clusters, dist):\n",
    "    n = len(clusters)\n",
    "    sigmas = [sigma(cluster, dist) for cluster in clusters]\n",
    "    rs = [[(sigmas[i] + sigmas[j])/dist_between_clusters(clusters[i], clusters[j], dist)\n",
    "           if i != j else 0\n",
    "          for i in range(n)]\n",
    "          for j in range(n)]\n",
    "    d = max(rs)\n",
    "    return sum(d)/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6614b40",
   "metadata": {},
   "source": [
    "**Dunn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "521d2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dunn(clusters, dist):\n",
    "    distances_between_clusters = [[dist_between_clusters(clusters[i], clusters[j], dist)\n",
    "                                   if i != j else inf\n",
    "                                   for j in range(i, len(clusters))]\n",
    "                                  for i in range(len(clusters))]\n",
    "    min_dist = min([min(one_clust_dist) for one_clust_dist in distances_between_clusters])\n",
    "    clust_sizes = [size_of_cluster(cluster, dist) for cluster in clusters]\n",
    "    max_size = max(clust_sizes)\n",
    "    return min_dist/max_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009d0aa2",
   "metadata": {},
   "source": [
    "## Zad 3 - stoplista"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80376b8",
   "metadata": {},
   "source": [
    "**Zliczanie słów**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e524ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    count_dict = {}\n",
    "    no_words = 0\n",
    "    for line in text:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            count_dict.setdefault(word, 0)\n",
    "            count_dict[word] += 1\n",
    "            no_words += 1\n",
    "    freq_dict = {key: val/no_words for key, val in count_dict.items()}\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d39689",
   "metadata": {},
   "source": [
    "**Tworzenie stoplisty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adebc239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stop_list(text, threshold):\n",
    "    freq_dict = count_words(text)\n",
    "    return set(map(lambda x: x[0], filter(lambda x: x[1] >sa threshold, list(freq_dict.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16232250",
   "metadata": {},
   "source": [
    "**Usuwanie wyrazów ze stop listy z oryginalnego tekstu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97af632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_elems(text, stop_list):\n",
    "    new_text = []\n",
    "    for line in text:\n",
    "        words = line.split()\n",
    "        valid_words = []\n",
    "        for word in words:\n",
    "            if word not in stop_list:\n",
    "                valid_words.append(word)\n",
    "        new_text.append(\" \".join(valid_words))\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e4d85",
   "metadata": {},
   "source": [
    "## Zad 4 - klasteryzacja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e5a46",
   "metadata": {},
   "source": [
    "**Wczytanie pliku**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e179c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(path):\n",
    "    with open(FILE_PATH, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d0365",
   "metadata": {},
   "source": [
    "**Tworzenie macierzy odległości**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abe9684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_of_similarities(dist, text):\n",
    "    return [[dist(line_x, line_y) for line_x in text] for line_y in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e9a70",
   "metadata": {},
   "source": [
    "**Klasteryzacja i stworzenie zbioru zbiorów znajdujących sie w jednym klastrze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cdb7ec5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cluster(text, measure, eps=1, min_sample=1):\n",
    "    sim_matrix = matrix_of_similarities(measure, text)\n",
    "    clusters = DBSCAN(eps=eps, min_samples=min_sample).fit_predict(sim_matrix)\n",
    "    temp_dict = {cluster: [] for cluster in clusters}\n",
    "    for i in range(len(text)):\n",
    "        temp_dict[clusters[i]].append(text[i])\n",
    "        \n",
    "    return list(temp_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f1422",
   "metadata": {},
   "source": [
    "## Zad 5 - Testy\n",
    "\n",
    "Algorytm klasteryzacji: DBSCAN (z parametrami eps=1 oraz min_sample=1)\n",
    "    \n",
    "Cosinus oraz DICE liczone są dla n-gramów długosci 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "310e766f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs                 \tStoplist threshold: None\n",
      "Dav-boul: 0.556470\tDunn: 0.691141\n",
      "Time of test: 200.25987 [s]\n",
      "\n",
      "dice_metric         \tStoplist threshold: None\n",
      "Dav-boul: 0.418169\tDunn: 0.880364\n",
      "Time of test: 1.59586 [s]\n",
      "\n",
      "cosine_metric       \tStoplist threshold: None\n",
      "Dav-boul: 0.425149\tDunn: 0.849912\n",
      "Time of test: 6.75103 [s]\n",
      "\n",
      "levenshtein_distance\tStoplist threshold: None\n",
      "Dav-boul: 0.499485\tDunn: 0.830428\n",
      "Time of test: 373.85223 [s]\n",
      "\n",
      "lcs                 \tStoplist threshold: 0.01\n",
      "Dav-boul: 0.630522\tDunn: 0.655169\n",
      "Time of test: 150.42608 [s]\n",
      "\n",
      "dice_metric         \tStoplist threshold: 0.01\n",
      "Dav-boul: 0.401262\tDunn: 0.852130\n",
      "Time of test: 1.98703 [s]\n",
      "\n",
      "cosine_metric       \tStoplist threshold: 0.01\n",
      "Dav-boul: 0.408359\tDunn: 0.803537\n",
      "Time of test: 7.54199 [s]\n",
      "\n",
      "levenshtein_distance\tStoplist threshold: 0.01\n",
      "Dav-boul: 0.389439\tDunn: 0.913655\n",
      "Time of test: 290.46506 [s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "def single_test(text, metric, stop_list_threshold=0.01):\n",
    "    working_text = text if stop_list_threshold is None \\\n",
    "            else del_elems(text, make_stop_list(text, stop_list_threshold))\n",
    "    \n",
    "    clusters = cluster(working_text, metric, eps=1, min_sample=1)\n",
    "    db_eval = davies_bouldin(clusters, metric)\n",
    "    dunn_eval = dunn(clusters, metric)\n",
    "    print(f'{metric.__name__:20s}\\tStoplist threshold: {stop_list_threshold}\\nDav-boul: {db_eval:5f}\\tDunn: {dunn_eval:5f}')\n",
    "\n",
    "    \n",
    "text = open_file(FILE_PATH)\n",
    "text = text[:125] # ograniczenie liczby linii\n",
    "\n",
    "k = 5\n",
    "metrics = [lcs, k_dice(k), k_cosine(k), levenshtein_distance]\n",
    "eval_metrics = [davies_bouldin, dunn]\n",
    "stoplist_thresholds = [None, 0.01]\n",
    "for stoplist_threshold in stoplist_thresholds:\n",
    "    for metric in metrics:\n",
    "        a = time()\n",
    "        single_test(text, metric, stoplist_threshold)\n",
    "        b = time()\n",
    "        print(f'Time of test: {round(b-a, 5)} [s]\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad 6\n",
    "Mam kilka pomysłów, dzięki którym być może można poprawic jakosć klasteryzacji:\n",
    "\n",
    "   1. **Lepsze dopasowanie parametrów** - w przestawionych powyżej testach uzyto jednego parametru do dopasowania klastrów. Można jednak użyć algorytmów ML do znalezienia najlepszego dopasowania, np. epsilona czy progu częstotliwości w stopliście.\n",
    "   \n",
    "   2. **Unifikacja zbioru wejściowego** - linie analizowanego tekstu posiadają wiele niespójności - ten sam adres czasem pisany jest z wielkiej litery, czasem w całości małymi albo wielkiemi symbolami. Oddala to znacząco teksty w  przedstawionych wyżej metrykach (np. LCS). Zmniejszenie wszystkich liter, usunięcie podwójnych spacji, być może również znaków interpunkcji - mogłoby pozytywnie wpłynąć na jakość klasyfikacji. Minusem jest oczywiscie lekka zmiana danych wejściowych, jednak moim zdaniem nie jest ona drastyczna i nie zmniejsza znacząco przejrzystości informacji zawartych w danych.\n",
    "   \n",
    "   3. **Postaranie się o lepsze dane** - wykorzystany wyżej DBSCAN wykorzystuje parametr `min_sample`, czyli najmniejsza liczba elementów w pojedynczym klastrze. W sytuacji, w której w naszym zbiorze wejściowym znajdowałyby się jedynie takie linie, co do których mamy pewność, że znajdują się im odpowiadające `k` linii, algorytm mógłby (przy równoczesnym ustaleniu większego epsilonu) uniknąć false_negative'ów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
